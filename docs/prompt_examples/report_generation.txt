SYSTEM
You are a British data integration and gouvernance advisor that gives advice about data integration and gouvernance to help a customer with data integration and gouvernance problems You spend alwayas a few sentences explaining the assumptions and reasoning behind the advice you then present. You use British English.

HUMAN
Based on the best practices and knowledge base and answers to multiple questions answered by a customer, please generate a series of at most 5 advices that are helpful to this customer to solve data integration and gouvernance issues if the customer has given enough information in his answers to your questions. So if the answers of the customer are too vague and lack detail or are not related to the questions you should refrain from giving advice.

Also include 3 pieces of advice about what the customer should avoid. In total you should give 5 pieces of advice regarding what should be done and 3 pieces of advice about what should be avoided.
Please also describe 3 potential positive outcomes in case the customer follows the suggested advices.
And highlight the important concepts with bold characters in your output using markdown syntax.

Make sure not to enumerate the advices if you decide to give them.
The knowledge base section starts with ==== KNOWLEDGE BASE START ==== and ends with ==== KNOWLEDGE BASE END ====.
The questions and answers section answered by the customer starts with ==== QUESTIONNAIRE ==== and ends with ==== QUESTIONNAIRE END ====.
==== KNOWLEDGE BASE START ====
Category: Produce & Aquire
Topic: Data Sources Trends

Data can be classified according to various attributes of data such as structured/semi-structured/un-
structured data based on how the data is structured or how the data entities are related or structured
relative to each other such as relational data. Based on the how fast the data is changing, data can be
classified as static data such as reference data, slowly changing data such as Master Data and slow
changing dimensional data, or volatile data such as time series data and transactional data from Point of
Sale (POS) systems. Similarly based on the velocity or speed of movement of data or how fast volatile
data is moved from the source to the destination, data can be classified as fast data that includes
streaming data. Streaming data could include social media data or data from IoT sensor data. Big Data
refers to not only large volumes of data but also includes fast data that becomes large volume when
accumulated over period of data. Big data includes variety of data including structure, semi-structured
and unstructured data. Based on how the data is ageing from the time when the data was generated,
data can be classified as operational data or historical data. Based on how frequently the data is
consumed, data can also be classified as hot, warm, and cold. Often, data within a system in an
organization is not exploited for analytics and such data is referred as dark data.
Modern Data Strategy is no longer limited to Big Data but is agnostic of the type of data and includes
ANY data whether its fast data or slow data, small data, or big data, transactional, operational, or
historical data. There is more and more emphasis on including dark data in scope and bringing it to the
data lake. Adoption of LLM for enterprises could hugely benefit from modern data strategy since it
includes consolidation of unstructured data on the data lake.

We live in a data-saturated world. Billions of interconnected devices communicate with countless cloud services. We accumulate data from server log files, GPS networks, security tools, call records, web traffic and more. Every digital transaction, from backend handoffs to the customer’s fingertips, is catalogued. Everything from the contents of a warehouse’s shelves to the temperature of our server rooms to the time and location of every login to our secure networks is recorded and stored somewhere. Most of it, today, is unstructured, untagged, untapped. In a word, useless.

Dark data represents data not collected and therefore not used. Considering that at least 80%
of data is dark,it represents the great hidden resource that flows untapped through major
organizations. Leveraging dark data in the data integration processes is a challenge as the
most sophisticated data sources—such as network transactions, IoT, mobility, Wi-Fi, or
industrial networks—require an advanced engine specially built for the purpose.
Dark data may be the biggest untapped resource in business today. Dark data, which includes the “data exhaust” generated as a byproduct of our online lives, is all the unknown and untapped data across your organization, generated by systems, devices and interactions. Maybe it’s siloed off somewhere; maybe the format or metadata is inconsistent. Maybe no one’s figured out what to do with it. Maybe you literally don’t know it exists. Most organizations struggle to capitalize on the full potential of their data. A full third of respondents report that more than 75 percent of their organization’s data is dark. Just 11 percent — one in nine — report that less than a quarter of their organization’s data is dark. Inadequate processes, resources and technology hamper the intelligent use of dark data. Neglected by business and IT managers, dark data is an underused asset that demands a more sophisticated approach to how organizations collect, manage and analyze information. Yet respondents also voiced hesitance about diving in. It’s a challenge, and an opportunity, of surprising scale. Machine data, a major source of dark data, is growing much faster than traditional organizational data, with an accelerating importance to decision making and organizational success. And because dark data can be a powerful fuel for artificial intelligence, organizations that fail to tap its power will also fail to keep up with, much less surpass, their competitors. And competition is global. Organizations must grasp the opportunities and confront the challenges of dark data — through greater strategic thinking, targeted technology investment, and more energetic and comprehensive skills training — to take full advantage of the next data revolution. Organizations need to think now about how to bring dark data into the light. All the unknown and untapped data across your company, generated by systems, devices and interactions. 

Why Data Stays Dark? 
Dark data contains records of all the activity and behavior of customers, users, transactions, applications, servers, networks and mobile devices. It includes configurations, message queues, the output of diagnostic commands, call detail records, sensor data from industrial systems, and more. Dark data, in short, is any data that isn’t being used. That includes the many types of data generated by an organization’s systems and applications, from machine data to server log files to customer and user data to sentiment analysis derived from social media. It’s the byproduct of day-to-day business activity, both within an organization and across the ecosystem of customers, partners and suppliers. It can be data that is considered too old or outdated to provide value, data in a format that can’t be accessed with the tools available to the organization, incomplete data or duplicate data — any data that needs to be “cleaned” before it can be used. Often, organizations ignore potentially valuable data because they don’t have the time or resources to prepare it for use. Or they may not understand its full potential. Or they may be bogged down in the status quo, meeting day-to-day requirements rather than looking ahead for opportunities. By definition, an organization’s dark data is the most difficult to access. How do organizations improve their access to dark data and the insights it holds? Traditionally, the quick and easy (if expensive) solution was to hire a consultancy and have them explore dark data in all the data sources by doing a data scan. Today’s dark data could one day be an accelerant for even greater AI performance. Thus, the advent of AI and the value of dark data go hand-in-hand. Dark data provides an enormous, untapped resource of information that AI can analyze. And AI-powered analytics tools can help make dark data ready for analysis on a scale that would be impossible with current methods. On the one hand, dark data and artificial intelligence hold almost unlimited potential to transform business and society, and working with data will be essential to virtually every job in their organizations. On the other hand, these business and IT leaders have low confidence in their own knowledge about AI and the data their company possesses. They’re even more skeptical of their colleagues’ and their organization’s readiness to take advantage of the potential both in their troves of untapped data and in the power of AI. Make “data driven” a reality, and make sure your approach to data helps organize and surface it in ways that will let you tap all of its potential. Understand your data. Commit to bringing more of it out of the dark, out of the gray areas, to be a vital part of your decision making. Data, like money, is an asset. Like money, a business has a fundamental responsibility to keep track of it, and use it to best advantage.


Category: Data Infrastructure, Governance, Security

Topic:  Data Governance
One common challenge with a data lake architecture is the lack of oversight on the contents of raw data stored in the data lake. Organizations need governance, semantic consistency, and access controls to avoid the pitfalls of creating a data swamp with no curation. Data catalogs play a crucial role in data governance by providing a centralized and organized repository of metadata and information about an organization's data assets. In active metadata management, metadata is not just stored and documented, it is continuously updated, enriched, and used in real-time to support various data management and data governance activities. The rich ecosystem of metadata connectors and dynamic discovery or crawling of data assets are key enablers for active metadata management. Data governance, focuses on the broader aspects of managing and governing data assets, including data quality, privacy, and compliance, while AI governance specifically addresses the ethical, transparent, and accountable use of AI technologies. These two governance areas often intersect and complement each other to ensure that AI systems are built on high-quality, compliant, and ethically managed data. 

Topic: Data Security
Data security primarily focuses on protecting data assets, including data at rest, data in transit, and data in use. It encompasses the confidentiality, integrity, and availability of data. Data masking, anonymization, tokenization, classification, and data privacy are important components of data security strategies aimed at protecting sensitive and confidential information. Data security encompasses a broader range of measures and practices aimed at protecting data assets throughout their lifecycle. AI security is a subset of data security that focuses specifically on the security of AI technologies, models, and algorithms. AI security deals with specific threats and risks associated with AI systems including machine learning models, algorithms, and AI applications. These threats may include adversarial attacks, model vulnerabilities, data poisoning, and model inversion attacks that target AI models and their operations. Data poisoning is a significant concern in AI/ML, especially when models are used in safety-critical applications like healthcare, finance, and autonomous systems.

Topic: Data Infrastructure
Edge computing moves data processing and analysis close to endpoints where data is generated, to deliver real-time responsiveness and reduce the cost associated with transferring large amounts of data. Edge computing can pre-process data, extract relevant information, and perform initial analytics locally. After this initial processing, summarized or relevant data can be sent to a data lake or edge network for further analysis, archival, and integration with historical data. Fog computing is a decentralized computing paradigm that extends cloud computing capabilities to the edge of the network, typically one or more hops away from the edge devices. It can be thought of as an intermediate layer between the edge devices and the centralized cloud. Organizations often require both real-time analytics at the edge and more extensive, historical analysis in a centralized data lake. Hybrid analytics solutions combine edge computing, fog computing with cloud or data lake-based analytics to provide a holistic view of data. It is important to conduct a thorough assessment of data lake, AI platform requirements and workloads to determine the appropriate cost effective infrastructure need for processing power. Emergence of hybrid cloud capabilities make it a valuable tool for organizations pursuing a comprehensive data lake strategy that spans multiple environments including edge, on-premise and multiple cloud service providers. Such capabilities include a cloud-centric container platform to run computationally intensive workloads by AI/ML applications utilizing CPU’s, GPU’s at scale. Most AI/ML workloads are cloud native in nature, deployed in containers and optimized to work with GPUs. They need to co-exist with other workloads on a shared infrastructure which results in a mix of virtual machines (VMs) and containers.

Operationalizing Data Lakes
While DevOps, DataOps and MLOps share a focus on automation, collaboration, and efficiency, they have distinct areas of emphasis. DevOps is primarily concerned with software development and deployment, while DataOps is focused on data-related processes and workflows. DataOps seeks to address challenges related to data integration, data quality, and data delivery, making it more tailored to the needs of data-driven organizations. MLOps is specifically focused on managing and operationalizing machine learning models, ensuring they perform well in production environments. By supporting interoperability of machine learning (ML) models, such as using the Open Neural Network Exchange (ONNX) format or PMML, ML models can be easily shared and deployed across different ML frameworks and inference engines.
Integrating DevOps, DataOps, and MLOps for a data lake requires a comprehensive strategy that promotes collaboration, automation, and efficient workflows across these disciplines. There is a need to effectively integrate DevOps, DataOps, and MLOps practices to create a cohesive and efficient data lake environment that supports data quality, data delivery, and machine learning model deployment in a streamlined and automated manner. 

==== KNOWLEDGE BASE END ====
==== QUESTIONNAIRE ====
Which area of your data ecosystem are you most concerned about?
Lack of skilled personnel - Missing skills in data science, analytics, AI and ML can impede the effective use of data.

What specific challenges are you facing due to the lack of skilled personnel in data science, analytics, AI, and ML?
 - We struggle to build and deploy machine learning models effectively.
 - Our data integration processes are inefficient and error-prone.
 - We have difficulty maintaining data quality and governance standards.

What are the primary data sources you are currently integrating, and what challenges do you face with these sources?
 - We are working with a mix of structured and unstructured data from IoT devices and social media platforms, and we encounter difficulties in data processing and storage.
 - Our primary data sources include external APIs and third-party data providers, and we struggle with data quality and reliability issues.

What specific issues do you encounter with data processing and storage when working with structured and unstructured data from IoT devices and social media platforms?
 - Data security and privacy concerns make it difficult to store and process data efficiently.
 - We lack the tools to effectively analyse and derive insights from unstructured data.

What specific data quality and governance standards are you finding difficult to maintain, and how are these impacting your operations?
 - We face difficulties in implementing data privacy and security measures, which affects our compliance with regulations.

What specific tools or technologies are you currently using for data integration, and what limitations or challenges have you encountered with them?
 - We are currently using custom scripts for data integration, which are error-prone and difficult to maintain.
 - We use Microsoft Azure Data Factory, but we struggle with real-time data processing and integration.

What are the main pain points you experience with your current data integration processes, and how do they impact your overall data strategy?
 - We struggle with real-time data integration, which impacts our ability to make timely and informed decisions.
 - The lack of automation in our data integration processes results in high manual effort and increased operational costs.
==== QUESTIONNAIRE END ====

Here is an example of a questionnaire with answers that are too vague to which you should not give advice:
==== QUESTIONNAIRE EXAMPLE ====
question: Which area of your data ecosystem are you most concerned about?
answer: Data Quality
question: What specific issues are you facing with data quality? Are they related to accuracy, consistency, completeness, or relevance of the data?
answer: Accuracy
question: What measures are currently in place to ensure data quality? Are you using any specific tools or methodologies for data quality management?
answer: We are using an MDM product.
==== QUESTIONNAIRE EXAMPLE END ====

Here is an example of a questionnaire with answers that have enough detail to which you should give advices:
==== QUESTIONNAIRE EXAMPLE ====
question: Which area of your data ecosystem are you most concerned about?
answer: Poor data quality
question: What measures are currently in place to ensure the quality of your data?
answer: At the moment we export the data from the billing, marketing and sales databases to our data lake using ETL jobs. The data is normalized, incorrect data is removed and deduplicated in this process.
question: Can you identify any specific sources or types of data where quality is particularly poor?
answer: Yes, customer data. It is often incorrect and has many duplicates.
question: Considering your concerns about the quality of customer data, have you considered implementing a data catalog to improve data literacy and discover underlying relationships?
answer: Yes, but we have not done so. Right now data lineage investigations are done manually by looking at the ETJ jobo logs.
question: Given the issues with data quality, particularly with customer data, have you explored the use of tools such as dbt (data build tool) or Apache SeaTunnel for data integration and improving data quality?
answer: Not yet, but I would like to know about it.
==== QUESTIONNAIRE EXAMPLE END ====

HUMAN
Tip: Make sure to answer in the correct format

HUMAN
Tip: Please make sure that you write all your answers in British English.