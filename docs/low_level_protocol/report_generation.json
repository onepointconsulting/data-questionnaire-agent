{
    "messages": [
        {
            "content": "You are a British data integration and gouvernance advisor that gives advice about data integration and gouvernance to help a customer with data integration and gouvernance problems You spend alwayas a few sentences explaining the assumptions and reasoning behind the advice you then present. You use British English.",
            "role": "system"
        },
        {
            "content": "Based on the best practices and knowledge base and answers to multiple questions answered by a customer, please generate a series of at most 5 advices that are helpful to this customer to solve data integration and gouvernance issues if the customer has given enough information in his answers to your questions. So if the answers of the customer are too vague and lack detail or are not related to the questions you should refrain from giving advice.\\n\\nAlso include 3 pieces of advice about what the customer should avoid. In total you should give 5 pieces of advice regarding what should be done and 3 pieces of advice about what should be avoided.\\nPlease also describe 3 potential positive outcomes in case the customer follows the suggested advices.\\nAnd highlight the important concepts with bold characters in your output using markdown syntax.\\n\\nMake sure not to enumerate the advices if you decide to give them.\\nThe knowledge base section starts with ==== KNOWLEDGE BASE START ==== and ends with ==== KNOWLEDGE BASE END ====.\\nThe questions and answers section answered by the customer starts with ==== QUESTIONNAIRE ==== and ends with ==== QUESTIONNAIRE END ====.\\n==== KNOWLEDGE BASE START ====\\nCategory: Data Infrastructure, Governance, Security\\n\\nTopic:  Data Governance\\nOne common challenge with a data lake architecture is the lack of oversight on the contents of raw data stored in the data lake. Organizations need governance, semantic consistency, and access controls to avoid the pitfalls of creating a data swamp with no curation. Data catalogs play a crucial role in data governance by providing a centralized and organized repository of metadata and information about an organization;s data assets. In active metadata management, metadata is not just stored and documented, it is continuously updated, enriched, and used in real-time to support various data management and data governance activities. The rich ecosystem of metadata connectors and dynamic discovery or crawling of data assets are key enablers for active metadata management. Data governance, focuses on the broader aspects of managing and governing data assets, including data quality, privacy, and compliance, while AI governance specifically addresses the ethical, transparent, and accountable use of AI technologies. These two governance areas often intersect and complement each other to ensure that AI systems are built on high-quality, compliant, and ethically managed data. \\n\\nTopic: Data Security\\nData security primarily focuses on protecting data assets, including data at rest, data in transit, and data in use. It encompasses the confidentiality, integrity, and availability of data. Data masking, anonymization, tokenization, classification, and data privacy are important components of data security strategies aimed at protecting sensitive and confidential information. Data security encompasses a broader range of measures and practices aimed at protecting data assets throughout their lifecycle. AI security is a subset of data security that focuses specifically on the security of AI technologies, models, and algorithms. AI security deals with specific threats and risks associated with AI systems including machine learning models, algorithms, and AI applications. These threats may include adversarial attacks, model vulnerabilities, data poisoning, and model inversion attacks that target AI models and their operations. Data poisoning is a significant concern in AI/ML, especially when models are used in safety-critical applications like healthcare, finance, and autonomous systems.\\n\\nTopic: Data Infrastructure\\nEdge computing moves data processing and analysis close to endpoints where data is generated, to deliver real-time responsiveness and reduce the cost associated with transferring large amounts of data. Edge computing can pre-process data, extract relevant information, and perform initial analytics locally. After this initial processing, summarized or relevant data can be sent to a data lake or edge network for further analysis, archival, and integration with historical data. Fog computing is a decentralized computing paradigm that extends cloud computing capabilities to the edge of the network, typically one or more hops away from the edge devices. It can be thought of as an intermediate layer between the edge devices and the centralized cloud. Organizations often require both real-time analytics at the edge and more extensive, historical analysis in a centralized data lake. Hybrid analytics solutions combine edge computing, fog computing with cloud or data lake-based analytics to provide a holistic view of data. It is important to conduct a thorough assessment of data lake, AI platform requirements and workloads to determine the appropriate cost effective infrastructure need for processing power. Emergence of hybrid cloud capabilities make it a valuable tool for organizations pursuing a comprehensive data lake strategy that spans multiple environments including edge, on-premise and multiple cloud service providers. Such capabilities include a cloud-centric container platform to run computationally intensive workloads by AI/ML applications utilizing CPU\\u2019s, GPU\\u2019s at scale. Most AI/ML workloads are cloud native in nature, deployed in containers and optimized to work with GPUs. They need to co-exist with other workloads on a shared infrastructure which results in a mix of virtual machines (VMs) and containers.\\n\\nOperationalizing Data Lakes\\nWhile DevOps, DataOps and MLOps share a focus on automation, collaboration, and efficiency, they have distinct areas of emphasis. DevOps is primarily concerned with software development and deployment, while DataOps is focused on data-related processes and workflows. DataOps seeks to address challenges related to data integration, data quality, and data delivery, making it more tailored to the needs of data-driven organizations. MLOps is specifically focused on managing and operationalizing machine learning models, ensuring they perform well in production environments. By supporting interoperability of machine learning (ML) models, such as using the Open Neural Network Exchange (ONNX) format or PMML, ML models can be easily shared and deployed across different ML frameworks and inference engines.\\nIntegrating DevOps, DataOps, and MLOps for a data lake requires a comprehensive strategy that promotes collaboration, automation, and efficient workflows across these disciplines. There is a need to effectively integrate DevOps, DataOps, and MLOps practices to create a cohesive and efficient data lake environment that supports data quality, data delivery, and machine learning model deployment in a streamlined and automated manner. \\n\\n\\nCategory: Data Infrastructure, Governance, Security\\nAdditional Category: Organise & Prepara\\nTopic: Data Quality\\n\\nWhat Is Data Quality? \\nData quality refers to the degree of accuracy, consistency, completeness, reliability, and relevance of the data collected, stored, and used within an organization or a specific context. High-quality data is essential for making well-informed decisions, performing accurate analyses, and developing effective strategies. Data quality can be influenced by various factors, such as data collection methods, data entry processes, data storage, and data integration. Maintaining high data quality is crucial for organizations to gain valuable insights, make informed decisions, and achieve their goals.\\n\\nWhy Is Data Quality Important?\\nHere are several reasons data quality is critical for organizations:\\n\\n1. Informed decision making: Low-quality data can result in incomplete or incorrect information, which negatively affects an organization\\u2019s decision-making process. With access to accurate and dependable data, business leaders can make informed decisions that promote growth and profitability.\\n2. Operational efficiency: Data quality has a direct influence on operational efficiency by providing all departments with the accurate information needed for everyday tasks, including inventory management and order processing. Improved data quality leads to reduced errors in these processes and increases productivity.\\n3. Customer satisfaction: Inaccurate customer records can make it more difficult to provide quality service to customers. Maintaining high-quality customer databases is crucial for improving satisfaction among existing clients.\\n4. Revenue opportunities: Data quality directly affects an organization\\u2019s bottom line by enabling more effective marketing strategies based on precise customer segmentation and targeting. By using high-quality data to create personalized offers for specific customer segments, companies can better convert leads into sales and improve the ROI of marketing campaigns.\\n\\n\\nData Quality vs. Data Integrity\\nData integrity concentrates on maintaining consistent data across systems while preventing unauthorized changes or corruption of information during storage or transmission. The primary focus of data integrity is protecting data from any unintentional or malicious modifications, whether it is in storage or transit.\\nKey differences between data quality and data integrity include:\\n1. Objective: While both concepts aim to improve overall trustworthiness in an organization\\u2019s information assets, their primary focus differs. Data quality targets specific attributes of individual records, while data integrity ensures reliability throughout the entire data lifecycle, including creation, update, deletion, storage, and transmission.\\n2. Methods: Enhancing data quality might involve cleansing, standardizing, enriching, or validating data elements, while preserving data integrity necessitates robust access controls, encryption measures, and backup/recovery strategies.\\n3. Scope: Data quality primarily deals with dataset content, while data integrity is more concerned with the overall system architecture and processes that ensure consistency across different platforms or applications.\\n\\nWhat are the six key elements that contribute to data quality?\\n1. Completeness: Is all the necessary data included in the dataset?  Completeness concerns whether a dataset contains all necessary records, without missing values or gaps. A complete dataset allows for more comprehensive analysis and decision-making. To improve the completeness, you can use techniques like imputing missing values, merging multiple information sources, or utilizing external reference datasets.\\n2. Consistency: Is the data consistent between sources and over time? Consistency measures the extent to which data values are coherent and compatible across different datasets or systems. Incorrect data can cause wrong conclusions and confusion among different users who rely on the information to make decisions. To improve consistency, you can implement data standardization techniques, such as using consistent naming conventions, formats, and units of measurement.\\n3. Validity: Does the data conform to the required format or structure? \\n4. Uniqueness: Are there any duplicates in the dataset? Uniqueness refers to the absence of duplicate records in a dataset. Duplicate entries can skew analysis by over-representing specific data points or trends. The primary action taken to improve the uniqueness of a dataset is to identify and remove duplicates. You can use automated deduplication tools to identify and eliminate redundant records from your database.\\n5. Accuracy: How correctly does the data reflect what occurred in the real world? Accuracy refers to the extent to which data accurately represents real-world values or events. Ensuring accuracy involves identifying and correcting errors in your dataset, such as incorrect entries or misrepresentations. One way to improve accuracy is by implementing data validation rules, which help prevent inaccurate information from entering your system.\\n6. Timeliness: Is the data available when it is expected and needed? Timeliness ensure that your data is up-to-date and relevant when used for analysis or decision-making purposes. Outdated information can lead to incorrect conclusions, so maintaining up-to-date datasets is essential. Techniques like incremental updates, scheduled refreshes, or real-time streaming can help keep datasets current.\\n\\nTo manage data quality effectively, we recommend focusing your efforts on the following five areas:\\n1.\\tOversight: Your data is a strategic asset for your organization, and it needs to be managed as such. Key stakeholders that depend on the quality of the data should meet on a regular basis to drive organizational accountability for maintaining data quality. Data governance initiatives may include both employee education on the importance of data quality, as well as the introduction of data policies and procedures to help avoid downstream data problems. \\n2.\\tValidate: Before the data is enriched or published to the data lake, it is preferable to verify its quality using data profiling and cleansing tools. It might also be helpful to establish data standards to make it easier to spot egregious issues. \\n3.\\tPrevent: A key preemptive measure to keep data clean is to create documentation around the data attributes or create a data dictionary. For example, it can be helpful to maintain a metric/dimension glossary that users can reference and establish naming conventions for datasets and their various sub-elements to keep them consistent over time. \\n4.\\tMonitor: It\\u2019s helpful to keep a watchful eye on the incoming data and setting up alerts to monitor key metrics or datasets. \\n5.\\tAudit & Fix: Once an issue has been identified, you need a systematic process for resolving data problems. Rather than introducing a Band-Aid solution, we recommend tracing the issue back to its root cause to avoid future issues that might appear due to a temporary fix.\\n\\nStrategies for Improving Data Quality\\n1. Establish Data Governance Policies\\nCreating data governance policies ensures uniformity in handling and managing data throughout your organization. These policies should outline roles, responsibilities, standards, and processes related to data management. Implementing clear guidelines on collecting, storing, processing, and sharing information within the company can, over time, significantly improve overall data quality.\\n\\n2. Offer Data Quality Training\\nProviding training programs focused on data quality management equips employees with the knowledge and skills needed to handle information responsibly. Regular workshops or seminars, covering topics like data collection practices or error detection techniques, will empower team members to contribute to high data quality standards.\\n\\n3. Keep Documentation Accurate and Up-to-Date\\nMaintaining current documentation about your data sources, processes, and systems helps users understand the context of the information they are working with. This documentation should include details about data lineage (how it was collected), transformations applied to it, and any assumptions made during analysis. Accurate documentation can help prevent misunderstandings that may lead to incorrect insights.\\n\\n4. Implement Data Validation Techniques\\nData validation techniques are essential to guarantee accurate input into your systems. Introducing checks like format validation (e.g., validating that email addresses are correct), range constraints (e.g., age limits), or referential integrity rules (e.g., foreign key constraints) helps prevent incorrect or inconsistent values from entering your databases.\\n\\n5. Implement Feedback Loops\\nFeedback loops involve gathering input from end-users regarding potential inaccuracies in datasets or reporting outputs. Fostering a culture of open communication around possible errors allows organizations to identify problems quickly and proactively implement necessary changes, rather than reacting after the fact when consequences may already have occurred.\\n\\n6. Use Data Cleansing Tools\\nData cleansing tools are designed to automatically identify errors in datasets by comparing them against predefined rules or patterns. These tools can also be used for tasks like removing duplicates from records or normalizing values according to specific criteria (e.g., capitalization). Regularly using these tools ensures that your systems store only high-quality information.\\n\\n7. Monitor Data Quality Metrics\\nMeasuring data quality metrics, such as completeness, accuracy, consistency, timeliness, or uniqueness, is crucial for identifying areas where improvements can be made. Regularly monitoring these metrics enables you to detect issues early on and take corrective actions before they affect business operations.\\n\\nWhat is data Obervability?\\nData observability, often referred to as data observability or data monitoring, is a practice in data management and data engineering that focuses on ensuring the reliability, quality, and performance of data pipelines and data-related processes. It involves the continuous monitoring, tracking, and analysis of data as it flows through various stages of data processing, storage, and transformation. The primary goal of data observability is to maintain data integrity, detect anomalies, and troubleshoot issues in real-time or near-real-time.\\n\\n\\nCategory: AI & ML foundations\\nTopic: AI governance\\n\\nWhat is AI governance?\\nAI is increasingly being used as an automated decision-making system for businesses. Some of these are straightforward predictions that do not require human oversight. Others are critical decisions that are made with little human oversight. In these situations, the risk and impact of an inaccurate prediction is high. As organisations calibrate the extent of human involvement in AI, it is also crucial to define what AI governance means for them.\\nAI governance is a framework and process for organisations to ensure that their AI systems work as intended, in accordance to customer expectations, organisational goals and societal laws and norms. When integrated with other parts of the organisation, decision tradeoffs can be made in view of overall compliance and risk management perspectives.\\n\\nA traditional data governance program oversees a range of activities, including data security, reference and master data management, data quality, data architecture, and metadata management. Now with growing adoption of data science, machine learning, and AI, there are new components that should also sit under the data governance umbrella. These are namely machine learning model management and Responsible AI governance,\\nWhat is need of AI Governance?\\nJust as the use of data is governed by a data governance program, the development and use of machine learning models in production requires clear, unambiguous policies, roles, standards, and metrics. A robust machine learning model management program would aim to answer questions such as: \\n1.\\tWho is responsible for the performance and maintenance of production machine learning models? \\n2.\\tHow are machine learning models updated and/or refreshed to account for model drift (deterioration in the model\\u2019s performance)? \\n3.\\tWhat performance metrics are measured when developing and selecting models, and what level of performance is acceptable to the business? \\n4.\\tHow are models monitored over time to detect model deterioration or unexpected, anomalous data and predictions? \\n5.\\tHow are models audited, and are they explainable to those outside of the team developing them?\\nHow does AI ethics or Responsible AI relate to AI governance?\\nAI governance and AI ethics are often discussed in tandem. AI ethics refers to a set of moral principles that help define the boundaries and responsibilities for action, especially in highly ambiguous situations. The use of AI will invariably bring about ethical dilemmas. From the boundaries of privacy, to the impact of social media algorithms on our mental and emotional wellbeing, ethical decisions need to be made by leaders, not machines. This entails a moral judgement on what is good, and what is harmful. For example: How much should we trade off profit and transparency? How do we define a fair decision making process? How far should we trust AI systems to make important decisions without humans-in-the-loop? What features (e.g. gender, age, income) are justifiable in the delivery of differential treatment or services? What data is being chosen to train models, and does this data have pre-existing bias in and of itself? What are the protected characteristics that should be omitted from the model training process (such as ethnicity, gender, age, religion, etc.)? How do we account for and mitigate model bias and unfairness against certain groups? How do we respect the data privacy of our customers, employees, users, and citizens? How long can we legitimately retain data beyond its original intended use? Are the means by which we collect and store data in line not only with regulatory standards, but with our own company\\u2019s standards?\\nAI governance can help leaders make the implications of these ethical decisions more transparent, by designing metrics to evaluate ethical trade-offs. For example, we can compare a model designed to have maximum effectiveness but no fairness constraints, versus another model that is designed to be fair, to see the relative drop in effectiveness.\\nHow does MLOps relate to AI governance? \\nMLOps (a compound of \\u201cmachine learning\\u201d and \\u201coperations\\u201d) has proved a useful foundation for the implementation of sound AI governance. MLOps is a machine learning practice that draws from DevOps approaches to increase visibility, automation and availability in machine learning systems. MLOps arose out of the recognition that it is challenging to build a performant machine learning engine and subsequently maintain them. \\nMLOps encourages: \\n1.\\tVisibility of metrics. Enabling the ability to monitor and understand the way your systems are functioning at every level. \\n2.\\tVersion control. Ensuring you have the ability to collaborate, iterate and roll-back where necessary. \\n3.\\tAutomation. Automating various tasks of data scientists and engineers so that they can focus on the creative aspects\\n\\nFive Keys to Defining a Successful AI Governance Strategy\\n1.\\tA Top-Down And Bottom-Up Strategy \\nEvery AI governance program needs executive sponsorship. Without strong support from leadership, it is unlikely a company will make the right changes to improve data security, quality, and management. At the same time, individual teams have to take collective responsibility for the data they manage and the analysis they produce. There needs to be a culture of continuous improvement and ownership of data issues. This bottom-up approach can only be achieved in tandem with top-down communications and recognition of teams that have made real improvements and can serve as an example to the rest of the organization.\\n\\n2.\\tBalance Between Governance and Enablement\\nGovernance shouldn\\u2019t be a blocker to innovation; rather, it should enable and support innovation. That means in many cases, teams need to make distinctions between proof-of-concepts, self-service data initiatives, and industrialized data products, as well as the governance needs surrounding each. Space needs to be given for exploration and experimentation, but teams also need to make a clear decision about when self-service projects or proof-of-concepts should have the funding, testing, and assurance to become an industrialized, operationalized solution.\\n\\n3.\\tQuality at its Heart \\nIn many companies, data products produced by data science and business intelligence teams have not had the same commitment to quality as traditional software development (through movements such as extreme programming and software craftsmanship). In many ways, this arose because five to ten years ago, data science was still a relatively new discipline, and practitioners were mostly working in experimental environments, not pushing to production. So while data science used to be the wild west, today, its adoption and importance has grown so much that standards of quality applied to software development need to be reapplied. Not only does the quality of the data itself matter now more than ever, but also data products need to have the same high standards of quality \\u2014 through code review, testing and continuous integration/continuous development (CI/CD) \\u2014 that traditional software does if the insights are to be trusted and adopted by the business at scale.\\n\\n4.\\tModel Management \\nAs machine learning and deep learning models become more widespread in the decisions made across industries, model management is becoming a key factor in any AI Governance strategy. This is especially true today as the economic climate shifts, causing massive changes in underlying data and models that degrade or drift more quickly. Continuous monitoring, model refreshes and testing are needed to ensure the performance of models meet the needs of the business. To this end, MLOps is an attempt to take the best of DevOps processes from software development and apply them to data science.\\n\\n5.\\tTransparency and Responsible AI \\nEven if, per the third component, data scientists write tidy code and adhere to high quality standards, they are still giving away a certain level of control to complex algorithms. In other words, it\\u2019s not just about quality of data or code, but making sure that models do what they\\u2019re intended to do. There is growing scrutiny on decisions made by machine learning models, and rightly so. Models are making decisions that impact many people\\u2019s lives every day, so understanding the implications of the decisions they make and making the models explainable is essential (both for the people impacted and the companies producing them). Open source toolkits such as Aequitas, developed by the University of Chicago, make it simpler for machine learning developers, analysts, and policymakers to understand the types of bias that machine learning models bring. Similarly, Dataiku can compute and display subpopulation analyses as a part of its end-to-end data science, machine learning, and AI platform offering, which help assess if models behave identically across subpopulations. Of course, in any case, it\\u2019s always up to the individual organization to establish what\\u2019s considered fair (or not) for its particular use case.\\n\\n\\nCategory: Data Infrastructure, Governance, Security\\nTopic: DataOps\\n\\n\\nDataOps presents organizations with a set of principles that when correctly implemented, help to solve many of the challenges data, business and operations teams encounter. While the benefits of DataOps are clear, the best implementation path can be unclear. \\n\\nDataOps describes a set of tools and processes that, improve the speed and agility of developing data and analytics solutions while improving overall data quality. Inspired by DevOps, DataOps uses a set of principles and best practices in order to achieve its objectives: \\n1.\\tAchieving high data quality with continuous error detection \\n2.\\tBuilding transparency by monitoring and measuring results \\n3.\\tStreamlining data ingestion through process automation \\n4.\\tImproving collaboration between delivery and business teams \\n5.\\tDelivering with shorter release cycles\\n\\nTo understand DataOps, we need to start by understanding the concept in context with other similar methodologies. DataOps shares many of the objectives and practices of MLOps and DevOps \\u2013 in fact, many organizations aspire to leverage multiple Ops cohesively to benefit from the natural synergies.\\n\\nOur approach is measuring and evaluating the efforts teams are spending on performing recurring data tasks and investigating data issues.\\nThe data or ops team is consumed with data promotions or releasing data engineering features. The team may also be slowed down on other repeatable activities which are part of the data engineering workflow including but not limited to creating test data and retrieving real production data to get tasks started. On the business side, the time spent by the data team for each feature release is perceived as a black box with limited perceived value, serving only to delay attainment of business objectives.\\nReducing Data Tasks: Your team will benefit greatly from automating sections of your data pipeline as well as reducing friction introduced by recurring data engineering tasks.\\nReducing Data Issues: The business consumers of the analytics outputs are frequently asking the team to investigate either real or perceived data issues. This can bog down either ops or data teams, forcing them to investigate high priority issues which can be critical for decision making or sensitive downstream processes. End-users view the data as unrecognizable and potentially loose trust in the analytics. Your team will benefit substantially from statistical structural data validation of both the input source data and output analytics data. In this scenario, we recommend focusing on one of the dimensions to bring immediate value to the most impactful pain points. \\n\\nIntroducing statistical and structural testing doesn\\u2019t have to be overly complicated or engineered. Both statistical and structural data testing can be done by simple queries and scripts. When integrated in data pipelines it provides continuous validation in ongoing delivery, and achieves the following objectives: \\n1.\\tImprove transparency between data and business teams by defining pipeline input and output expectations \\n2.\\tDetect potential data anomalies before production release to enable proactive investigation\\n3.\\tIncrease data pipeline reliability by validating data sources are in the correct formats and within the expected range of values \\n4.\\tReduce breaks and errors by rejecting incompatible and erroneous data\\n\\nWe can continue to improve upon the data pipeline by implementing a quantitative and statistical evaluation to assess whether the results of our data transformation or aggregation output meets consumer expectations. The data pipeline workflow can be engineered to notify the Ops team where a data load doesn\\u2019t meet these threshold criteria (tip: don\\u2019t send notifications for regular, successful events to avoid notification fatigue). Naturally, the above rules we\\u2019ve defined will become part of a regular review cycle to ensure they are kept in alignment with end-user expectations and expanded as new pipeline features are developed.\\nIn addition to the rule and threshold-based validation strategy, implementing a statistical validation of data provides significant benefits.\\n\\nThe Apache incubator, open-source projects and DataOps focused platforms are advancing the tool and technology landscape by providing continuous integration and development features for data projects. Data processing workflow & orchestrations as code, native source control integration for data structures and automation of data promotion processes are all contributing to this advancement. Existing progress is also encouraging new approaches to developing data pipelines often adopted by and integrated into existing products. Ultimately, the industry is evolving to achieve many of the desired outcomes listed are also objectives of DevOps. There are however substantial differences introduced with data centric applications. \\n1.\\tVisibility & Sensitivity \\nA large portion of data analytics is used by systems or humans as input into varying levels of decision making. It can be argued that the use of data analytics for decision making purposes, inherently makes these types of solutions intrinsically sensitive to data issues. In many cases, the data validation process is partially composed of a subjective evaluation to determine if the data falls into a feel-good range. High volume data projects make this type of validation exercise significantly challenging. \\n2.\\tApplication State \\nMost data analytics applications (as a whole) can be classified as stateful, since the output typically reflects a set of results for a specific context for a moment in time. Consequently, the change of state caused by an ongoing data refresh (sometimes happening on a frequent basis in production), or new feature release, has the potential to negatively impact the validity of the data. The results presented to the consumer could be dramatically different over the course of days, minutes or even seconds. Volatility in the results presented to a user in support of critical business decisions can significantly diminish their trust in the solution. \\n3.\\tData Engineering Workflow \\nDataOps pipeline typically consists of several stages that collectively manage and streamline the end-to-end data operations process. These stages help ensure that data is efficiently and reliably delivered from source systems to its destination, whether for analytics, reporting, or other purposes. While the specific stages may vary depending on the organization and its data needs, the following are common stages in a DataOps pipeline: data extraction, data transformation, data ingestion, data quality checks and validation, data storage, data cataloging and metadata management, data access and security, data consumption, data monitoring and alerts, data versioning, testing, deployment automation.\\n4.\\tQuantitative Testing Benefits\\nA larger proportion of the output from data analytics applications is numerical. Using a statistical approach to validate the application offers greater value (in most cases) than in software development projects.\\n\\n==== KNOWLEDGE BASE END ====\\n==== QUESTIONNAIRE ====\\nWhich area of your data ecosystem are you most concerned about?\\nCompliance and security risks - Mishandling data can lead to legal troubles and reputational damage.\\n\\nWhat specific challenges are you facing regarding compliance and security in your data management practices?\\n - We struggle with ensuring data is handled according to regulations, leading to potential legal issues.\\n\\nWhat specific aspects of data governance are causing you the most concern in relation to compliance and security?\\n - We are uncertain about how to implement data governance policies effectively.\\n\\nWhat specific pain points do you experience when trying to implement data governance policies that ensure compliance with regulations?\\n - We find it challenging to align data governance policies with existing regulations.\\n\\nWhat specific areas of data governance do you believe require more clarity or support to ensure compliance with regulations?\\n - It;s unclear how to effectively monitor and audit our data practices to ensure adherence to regulations.\\n\\nWhat specific measures have you taken to monitor and audit your data management practices for compliance with regulations?\\n - We have implemented a basic monitoring system, but it lacks comprehensive coverage.\\n\\nWhat specific data governance practices do you feel are necessary to enhance compliance and security in your organisation?\\n\\n==== QUESTIONNAIRE END ====\\n\\nHere is an example of a questionnaire with answers that are too vague to which you should not give advice:\\n==== QUESTIONNAIRE EXAMPLE ====\\nquestion: Which area of your data ecosystem are you most concerned about?\\nanswer: Data Quality\\nquestion: What specific issues are you facing with data quality? Are they related to accuracy, consistency, completeness, or relevance of the data?\\nanswer: Accuracy\\nquestion: What measures are currently in place to ensure data quality? Are you using any specific tools or methodologies for data quality management?\\nanswer: We are using an MDM product.\\n==== QUESTIONNAIRE EXAMPLE END ====\\n\\nHere is an example of a questionnaire with answers that have enough detail to which you should give advices:\\n==== QUESTIONNAIRE EXAMPLE ====\\nquestion: Which area of your data ecosystem are you most concerned about?\\nanswer: Poor data quality\\nquestion: What measures are currently in place to ensure the quality of your data?\\nanswer: At the moment we export the data from the billing, marketing and sales databases to our data lake using ETL jobs. The data is normalized, incorrect data is removed and deduplicated in this process.\\nquestion: Can you identify any specific sources or types of data where quality is particularly poor?\\nanswer: Yes, customer data. It is often incorrect and has many duplicates.\\nquestion: Considering your concerns about the quality of customer data, have you considered implementing a data catalog to improve data literacy and discover underlying relationships?\\nanswer: Yes, but we have not done so. Right now data lineage investigations are done manually by looking at the ETJ jobo logs.\\nquestion: Given the issues with data quality, particularly with customer data, have you explored the use of tools such as dbt (data build tool) or Apache SeaTunnel for data integration and improving data quality?\\nanswer: Not yet, but I would like to know about it.\\n==== QUESTIONNAIRE EXAMPLE END ====\\n\\n",
            "role": "user"
        },
        {
            "content": "Tip: Make sure to answer in the correct format",
            "role": "user"
        },
        {
            "content": "Tip: Please make sure that you write all your answers in British English.",
            "role": "user"
        }
    ],
    "model": "gpt-4o-mini",
    "stream": false,
    "n": 1,
    "temperature": 0.5,
    "logprobs": false,
    "tools": [
        {
            "type": "function",
            "function": {
                "name": "ConditionalAdvice",
                "description": "If there is enough information to give advice then advice will be available here.",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "has_advice": {
                            "description": "Whether there is advice here or not",
                            "type": "boolean"
                        },
                        "advices": {
                            "description": "In case there is enough information to give advice, this list will contain advice to give to the user",
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        },
                        "what_you_should_avoid": {
                            "description": "A list of advice about what you should not do and avoid.",
                            "default": [],
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        },
                        "positive_outcomes": {
                            "description": "A list of potential positive outcomes in case the user follows the advice.",
                            "default": [],
                            "type": "array",
                            "items": {
                                "type": "string"
                            }
                        },
                        "confidence": {
                            "description": "The confidence rating at the time of the report.",
                            "allOf": [
                                {
                                    "title": "ConfidenceRating",
                                    "description": "Represents a rating of how confident the model is to give advice to a customer based on a questionnaire",
                                    "type": "object",
                                    "properties": {
                                        "id": {
                                            "title": "Id",
                                            "description": "The identifier of this session configuration",
                                            "type": "integer"
                                        },
                                        "reasoning": {
                                            "title": "Reasoning",
                                            "description": "The models;s reasoning behind the confidence rating.",
                                            "type": "string"
                                        },
                                        "rating": {
                                            "description": "The confidence rating of the model to give advice to a customer based on a questionnaire",
                                            "allOf": [
                                                {
                                                    "title": "ConfidenceDegree",
                                                    "description": "An enumeration.",
                                                    "enum": [
                                                        "outstanding",
                                                        "high",
                                                        "medium",
                                                        "mediocre",
                                                        "low"
                                                    ],
                                                    "type": "string"
                                                }
                                            ]
                                        }
                                    },
                                    "required": [
                                        "reasoning",
                                        "rating"
                                    ]
                                }
                            ]
                        }
                    },
                    "required": [
                        "has_advice",
                        "advices"
                    ]
                }
            }
        }
    ],
    "parallel_tool_calls": false,
    "tool_choice": {
        "type": "function",
        "function": {
            "name": "ConditionalAdvice"
        }
    }
}